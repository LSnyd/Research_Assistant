{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lisa/miniconda3/envs/tensorboard/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import  GPT2Tokenizer, GPT2LMHeadModel\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/lisa/.cache/huggingface/datasets/CShorten___csv/CShorten--ML-ArXiv-Papers-0dcddd7fc76c9211/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    }
   ],
   "source": [
    "arxiv_dataset = (load_dataset(path=\"CShorten/ML-ArXiv-Papers\", split='train').train_test_split(train_size=90000, test_size=10000))\n",
    "arxiv_dataset = DatasetDict({\n",
    "    'train': arxiv_dataset['train'],\n",
    "    'validation': arxiv_dataset['test']})\n",
    "\n",
    "abstracts = [arxiv_dataset['validation'][0]['abstract']]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract:\n",
      "\n",
      "  This paper investigates the influence of different acoustic features, audio-events based features and automatic speech translation based lexical features in complex emotion recognition such as curiosity. Pretrained networks, namely, AudioSet Net, VoxCeleb Net and Deep Speech Net trained extensively for different speech based applications are studied for this objective. Information from deep layers of these networks are considered as descriptors and encoded into feature vectors. Experimental results on the EmoReact dataset consisting of 8 complex emotions show the effectiveness, yielding highest F1 score of 0.85 as against the baseline of 0.69 in the literature. \n",
      "\n",
      "Generated Title: Identifying hidden states from data is of great importance in the prediction and management\n",
      "\n",
      "Generated Title: Learning to recognize visual objects without having hand-crafted labels: a novel approach\n",
      "\n",
      "Generated Title: On the Interpretability of Decision Trees for Deep Learning: A Case Study \n",
      "\n",
      "Generated Title: Learning to Walk: A case study of two-directional mobility from stationary\n",
      "\n",
      "Generated Title: Estimating the likelihood of a path given an input vector is crucial for many\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# trained model loading\n",
    "model_articles_path = './model_title_from_abstract'\n",
    "\n",
    "\n",
    "research_helper_tokenizer  = GPT2Tokenizer.from_pretrained(model_articles_path)\n",
    "research_helper_model = GPT2LMHeadModel.from_pretrained(model_articles_path).to(device)\n",
    "\n",
    "# define tokens \n",
    "bos = '<|endoftext|>'\n",
    "eos = '<|EOS|>'\n",
    "pad = '<|pad|>'\n",
    "body = '<|body|>'\n",
    "additional_special_tokens = [body]\n",
    "\n",
    "special_tokens_dict = {'eos_token': eos, 'bos_token': bos, 'pad_token': pad,'sep_token': body}\n",
    "\n",
    "# add tokes to tokenizer\n",
    "num_added_toks = research_helper_tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "articles = {}\n",
    "for abstract_raw in abstracts:\n",
    "    print(\"Abstract:\")\n",
    "    print()\n",
    "    print(abstract_raw.replace(\"\\n\",\" \"))\n",
    "    abstract_input = [bos, abstract_raw.replace(\"\\n\",\" \"),  body]\n",
    "\n",
    "    abstract_ids = research_helper_tokenizer.encode(abstract_input, return_tensors = 'pt').to(device)\n",
    "\n",
    "    generated_titles = research_helper_model.generate(\n",
    "        abstract_ids, \n",
    "        max_length= 18,  \n",
    "        num_return_sequences= 5,\n",
    "        no_repeat_ngram_size= 2,\n",
    "        repetition_penalty= 1.5,\n",
    "        top_p= 0.92,\n",
    "        temperature= .85,\n",
    "        do_sample= True,\n",
    "        top_k= 125,\n",
    "        early_stopping= True\n",
    "    )\n",
    "\n",
    "    for t in generated_titles:\n",
    "        text = research_helper_tokenizer.decode(t, skip_special_tokens=True)\n",
    "        print()\n",
    "        print(\"Generated Title:\", text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorboard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a44a7dfa71ff121c1b32add23bf472096244eb5361d8ac3b61007c87c3bb2ea6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
